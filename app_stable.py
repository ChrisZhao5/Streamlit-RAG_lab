import streamlit as st
import os
import tempfile

# --- å¯¼å…¥åº“ ---
try:
    from langchain_community.document_loaders import PyPDFLoader
    from langchain_community.vectorstores import FAISS
    # ä½¿ç”¨ HuggingFace æœ¬åœ°æ¨¡å‹ (å…è´¹ã€ç¨³å®šã€ä¸ç”¨è”ç½‘)
    from langchain_community.embeddings import HuggingFaceEmbeddings
    # ä½¿ç”¨ Google Gemini å›ç­”é—®é¢˜
    from langchain_google_genai import ChatGoogleGenerativeAI
    from langchain_text_splitters import RecursiveCharacterTextSplitter
    from langchain.chains import RetrievalQA
except ImportError as e:
    st.error(f"ç¯å¢ƒç¼ºå°‘åº“ï¼Œè¯·è¿è¡Œ: pip install sentence-transformers \n é”™è¯¯è¯¦æƒ…: {e}")
    st.stop()

# 1. é¡µé¢è®¾ç½®
st.set_page_config(page_title="Solaria Labs RAG Demo", layout="wide")
st.title("ğŸ¤– Bohan's RAG Prototype (Powered by Gemini 3.0)")

# 2. ä¾§è¾¹æ 
with st.sidebar:
    st.header("Configuration")
    api_key = st.text_input("Google API Key", type="password", help="Starts with AIza...")
    st.markdown("---")
    st.markdown("**Tech Stack:**\n- **Embeddings:** HuggingFace (Local)\n- **LLM:** Gemini 3 Flash (Cloud)\n- **Vector DB:** FAISS")

# 3. æ–‡ä»¶ä¸Šä¼ 
uploaded_file = st.file_uploader("Upload Document (PDF only)", type="pdf")

if uploaded_file and api_key:
    os.environ["GOOGLE_API_KEY"] = api_key
    
    # --- çŠ¶æ€ A: å¤„ç†æ–‡æ¡£ (æœ¬åœ°è¿è¡Œ) ---
    if "vectorstore" not in st.session_state:
        with st.spinner("ğŸš€ Processing Document with Local CPU..."):
            try:
                # A. ä¿å­˜ä¸´æ—¶æ–‡ä»¶
                with tempfile.NamedTemporaryFile(delete=False, suffix=".pdf") as tmp_file:
                    tmp_file.write(uploaded_file.getvalue())
                    tmp_file_path = tmp_file.name

                # B. åŠ è½½
                loader = PyPDFLoader(tmp_file_path)
                documents = loader.load()

                # C. åˆ‡åˆ†
                text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
                docs = text_splitter.split_documents(documents)

                # D. å‘é‡åŒ– (ä½¿ç”¨æœ¬åœ°æ¨¡å‹ all-MiniLM-L6-v2)
                # è¿™ä¸€æ­¥ä¸éœ€è¦ Keyï¼Œå®Œå…¨åœ¨æœ¬åœ°è·‘
                embeddings = HuggingFaceEmbeddings(model_name="all-MiniLM-L6-v2")
                
                # å­˜å…¥ Session State é˜²æ­¢æ¯æ¬¡æé—®éƒ½é‡è·‘
                st.session_state.vectorstore = FAISS.from_documents(docs, embeddings)
                
                st.success("âœ… Knowledge Base Ready!")
                os.remove(tmp_file_path)

            except Exception as e:
                st.error(f"Error initializing RAG: {e}")
                st.stop()

    # --- çŠ¶æ€ B: é—®ç­”ç•Œé¢ (è°ƒç”¨äº‘ç«¯ Gemini 3) ---
    query = st.text_input("What would you like to know?")

    if query:
        with st.spinner("ğŸ¤– Gemini 3 is Thinking..."):
            try:
                # E. åˆå§‹åŒ– LLM (å…³é”®ä¿®æ”¹ç‚¹ï¼)
                # ä½¿ç”¨ä½ åˆ—è¡¨é‡Œçš„çœŸå®æ¨¡å‹åç§°
                llm = ChatGoogleGenerativeAI(
                    model="models/gemini-3-flash-preview", 
                    temperature=0
                )
                
                # F. æ„å»ºæ£€ç´¢é“¾
                qa_chain = RetrievalQA.from_chain_type(
                    llm=llm,
                    chain_type="stuff",
                    retriever=st.session_state.vectorstore.as_retriever(),
                    return_source_documents=True
                )
                
                # G. æé—®
                result = qa_chain.invoke({"query": query})
                
                st.markdown("#### Answer:")
                st.info(result["result"])
                
                with st.expander("Show Source Context"):
                    for doc in result["source_documents"]:
                        st.text(f"Page {doc.metadata.get('page', '?')}:")
                        st.write(doc.page_content[:300] + "...")

            except Exception as e:
                st.error(f"Error: {e}")

else:
    st.info("ğŸ‘ˆ Enter Google API Key to start.")